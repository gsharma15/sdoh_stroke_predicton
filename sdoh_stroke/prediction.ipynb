{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6CAGsFbxQ7d"
      },
      "outputs": [],
      "source": [
        "#installing required packages\n",
        "!pip install shap\n",
        "!pip install catboost\n",
        "!pip install lime\n",
        "!pip install interpret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goma9K53xJNf"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for data manipulation, machine learning models, and evaluation metrics.\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shap\n",
        "import logging\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.under_sampling import NearMiss\n",
        "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import OneHotEncoder, KBinsDiscretizer, StandardScaler\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# Suppress warnings to clean output, including deprecation warnings which are common with evolving libraries.\n",
        "warnings.filterwarnings('ignore')\n",
        "matplotlib.use('Agg')  # Use Agg backend for matplotlib to enable plot generation without a GUI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-F19WKRxMgb"
      },
      "outputs": [],
      "source": [
        "# Define the path to the dataset. \n",
        "path = 'data_renamed.csv'\n",
        "\n",
        "# Load the dataset into a pandas DataFrame and read the CSV file located at the specified path and load its contents into the DataFrame 'df'.\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# Access the 'stroke' column of the DataFrame. This column contains the target variable we aim to predict.\n",
        "df['stroke']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zTqHHK7Hyp1H"
      },
      "outputs": [],
      "source": [
        "class StrokePrediction:\n",
        "    def __init__(self, data_path, mode):\n",
        "        # Load the dataset from a specified path.\n",
        "        self.data = pd.read_csv(data_path,  encoding ='latin1', sep =',')\n",
        "        #self.data = self.data[:1000]\n",
        "\n",
        "        # Print the initial shape (number of rows and columns) of the dataset for a quick overview of its size.\n",
        "        print(self.data.shape)\n",
        "        # Create a deep copy of the original data for any operations that might require reference to the unmodified dataset.\n",
        "        self.data_original = self.data.copy()\n",
        "\n",
        "        # Initialize a dictionary to store models, an empty list for top features selected by the models,\n",
        "        # and a dictionary to store ROC AUC scores for model evaluation.\n",
        "        self.models = {}\n",
        "        self.top_features = []\n",
        "        self.mode = mode # Mode specifies the preprocessing strategy to use.\n",
        "        self.roc_auc = {}\n",
        "\n",
        "        # Handle different data preprocessing strategies based on the 'mode'.\n",
        "        if self.mode=='undersample':\n",
        "            # For 'undersample' mode, use NearMiss algorithm to balance the dataset by undersampling the majority class.\n",
        "            # This can help in cases where the dataset is highly imbalanced towards one class.           \n",
        "            X = self.data.drop('stroke', axis=1) # Features (exclude 'stroke' column).\n",
        "            y = self.data['stroke'] # Target variable.\n",
        "\n",
        "            undersample = NearMiss(version=3, n_neighbors_ver3=3)\n",
        "            # Transform the dataset\n",
        "            X, y = undersample.fit_resample(X, y)\n",
        "            # Print the class distribution after undersampling to verify the balancing.\n",
        "            a,b = np.unique(y, return_counts=True)\n",
        "            print(a,b)\n",
        "\n",
        "        elif self.mode=='smote_nc':\n",
        "            # For 'smote_nc' mode, use SMOTENC for oversampling, particularly useful when categorical features are present.\n",
        "            # SMOTENC can handle both continuous and categorical features, addressing class imbalance by generating synthetic samples.\n",
        "            X = self.data.drop('stroke', axis=1)\n",
        "            y = self.data['stroke']\n",
        "            # List of categorical feature names that will be treated specially by SMOTENC.\n",
        "            cat_features = ['DHH_MS', 'DHH_AGE', 'Working','Working_Last_Week','Regular_Healthcare_Provider', 'Urban_Rural','Household_Ownership','Problem_Handling_Ability',\n",
        "            'Day_to_Day_Demands_Ability','Source_of_Stress','Perceived_Health','Perceived_Mental_Health','Perceived_Life_Stress','Food_Security', 'Food_Affordability','Sense_of_Community_Belonging',\n",
        "            'Level_of_Education','Drank_Alcohol_12mo','Drank_Alcohol_freq','Drank_more_than_4_once_freq','Asthma','High_BP','High_BP_took_meds','Diabetes',\n",
        "            'Heart_Disease','High_Cholesterol_took_meds','Smoked_more_than_100_lifetime','Smoker_Type', 'Self_percieved_weight','Expose_to_smoke_pvt_vehicle',\n",
        "            'Expose_to_smoke_public', 'Mood_Disorder','Anxiety_Disorder','Mental_Health_Consult_12mo','White','Chinese','South_Asian','Black','Filipino','Latin_America',\n",
        "            'South_East_Asian','Arab','West_Asian','Japanese','Korean','Canadian','French','English','German','Scottish','Irish','Italian',\n",
        "            'Ukrainian','Dutch','Chinese_Ethnic','Jewish','Polish','South_Asian_Ethnic','Norwegian', 'Welsh','Swedish','Other','Metis','Inuit']\n",
        "            \n",
        "            smote_nc = SMOTENC(categorical_features=cat_features, random_state=0)\n",
        "            X, y = smote_nc.fit_resample(X, y)\n",
        "        else:\n",
        "            #data['DHH_SEX'] = data['DHH_SEX'].map({'Male': 0, 'Female': 1})\n",
        "            # Default preprocessing involves handling missing values using KNNImputer.\n",
        "            # This is particularly useful in datasets where dropping rows with missing values might lead to significant data loss.\n",
        "            imputer = KNNImputer(n_neighbors=5) # Imputes missing values based on the 5 nearest neighbors.\n",
        "            transformed_data = imputer.fit_transform(self.data)\n",
        "            # Replace the original dataset with the imputed one, maintaining the same structure and column names.\n",
        "            self.data = pd.DataFrame(transformed_data, columns=self.data.columns)\n",
        "            X = self.data.drop('stroke', axis=1) # Features\n",
        "            y = self.data['stroke'] # Target variable\n",
        "\n",
        "        # Split the dataset into training and testing sets, with 20% of the data reserved for testing.\n",
        "        # Stratify parameter ensures that the proportion of classes in the dataset is preserved in both training and testing sets.\n",
        "        # This is crucial for maintaining a representative distribution of classes, especially in imbalanced datasets.\n",
        "        #self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "        #preprocessed_data = pd.concat([X, pd.Series(y, name='stroke')], axis=1)\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "\n",
        "    def explain_with_lime(self, model, X_train, X_test, model_name):\n",
        "        # Initialize the LIME explainer with the training data. This explainer will be used to generate explanations\n",
        "        # for predictions made by the model, providing insights into which features contribute most to each prediction.\n",
        "        explainer = LimeTabularExplainer(\n",
        "            training_data=self.X_train.values,  # The training dataset used for model fitting.\n",
        "            feature_names=self.X_train.columns,  # Names of the features in the dataset for interpretability.\n",
        "            class_names=['No Stroke', 'Stroke'],  # The outcome classes for the binary classification task.\n",
        "            verbose=True,  # Enables detailed logging of the explanation generation process.\n",
        "            mode='classification'  # Specifies that the model is being used for classification.\n",
        "        )\n",
        "\n",
        "        # Select a random instance from the test dataset to explain. This helps in understanding how the model makes predictions for individual data points.\n",
        "        instance = self.X_test.sample(n=1).values[0]\n",
        "\n",
        "        # Generate an explanation for the selected instance using LIME. This explanation will highlight the top features that influence the model's prediction for this particular data point.\n",
        "        exp = explainer.explain_instance(\n",
        "            data_row=instance,  # The data point to explain.\n",
        "            predict_fn=model.predict_proba,  # The prediction function of the model (probability outputs required).\n",
        "            num_features=5  # Limit the explanation to the top 5 features for clarity.\n",
        "        )\n",
        "\n",
        "        # Generate a visual plot of the explanation with matplotlib, showing the contribution of each feature to the model's prediction.\n",
        "        fig = exp.as_pyplot_figure()\n",
        "\n",
        "        # Predict the class for the selected instance to add context to the explanation plot.\n",
        "        predicted_class = model.predict(instance.reshape(1, -1))[0]\n",
        "        predicted_class_name = 'Stroke' if predicted_class else 'No Stroke'\n",
        "\n",
        "        # Customize the plot for better readability and understanding.\n",
        "        fig.set_size_inches(10, 6)  # Adjust figure size for visibility.\n",
        "        plt.xlabel('Important Features')  # Label for the x-axis.\n",
        "        plt.ylabel('Contribution to Prediction')  # Label for the y-axis.\n",
        "        plt.title(f\"Visualization of the local contribution of features in classifying a single test instance (predicted class = {predicted_class_name}) using the {model_name} model.\")\n",
        "        plt.xticks(rotation=45)  # Rotate the x-axis labels for better readability.\n",
        "        plt.tight_layout()  # Adjust the layout to make sure everything fits without overlap.\n",
        "\n",
        "        # Save the generated plot to a file. \n",
        "        filename = f\"{model_name}_lime_explanation.png\"\n",
        "        plt.savefig(filename)\n",
        "        plt.show()  # Display the plot in the notebook\n",
        "\n",
        "\n",
        "    def plot_feature_importance(self, model, feature_names, model_name):\n",
        "        # Retrieve the feature importances from the model. This assumes the model has a `feature_importances_` attribute,\n",
        "        # common in tree-based models like RandomForest, GradientBoosting, and XGBoost.\n",
        "        importances = model.feature_importances_\n",
        "        # Sort the indices of the importances in ascending order and select the top 15 features for visualization.\n",
        "        indices = np.argsort(importances)[-15:]\n",
        "        plt.figure(figsize=(10, 7)) # Adjust figure size for visibility.\n",
        "        plt.title(f\"Top 15 Feature Importances for {model_name}\")\n",
        "        # Create a horizontal bar chart to visualize the relative importance of the top 15 features.\n",
        "        # The y-axis is the range of the number of features, and the x-axis represents their relative importance.   \n",
        "        plt.barh(range(len(indices)), importances[indices], align=\"center\")\n",
        "        plt.yticks(range(len(indices)), [feature_names[i] for i in indices]) # Set the y-ticks to correspond to the feature names, using the indices to map back to the original names.\n",
        "        plt.xlabel(\"Relative Importance\") # Label for the x-axis.\n",
        "\n",
        "        # Save the generated plot to a file. \n",
        "        filename = f\"{model_name}_feature_importance.png\"\n",
        "        plt.savefig(filename)\n",
        "        plt.show() # Display the plot in the notebook\n",
        "\n",
        "\n",
        "    def train_model(self):\n",
        "        # Define a dictionary mapping model names to their respective classifier instances.\n",
        "        # This approach allows for easy expansion or modification of the model lineup.\n",
        "        model_definitions = {\n",
        "            'RandomForest': RandomForestClassifier(),  # Ensemble method based on decision trees.\n",
        "            'GradientBoosting': GradientBoostingClassifier(),  # Boosting method for high performance.\n",
        "            'KNN': KNeighborsClassifier(),  # A simple, distance-based classifier.\n",
        "            'NeuralNetwork': MLPClassifier(max_iter=1000),  # Multi-layer Perceptron classifier.\n",
        "            'LGBM': LGBMClassifier(),  # LightGBM model, efficient for large datasets and high performance.\n",
        "            'CatBoost': CatBoostClassifier(verbose=0),  # CatBoost, optimized for categorical data.\n",
        "            'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')  # XGBoost, known for performance and flexibility.\n",
        "        }\n",
        "\n",
        "        # Define a dictionary to hold the hyperparameters grids for each model. \n",
        "        # These grids specify the parameters to be explored during hyperparameter tuning, allowing for systematic experimentation to find the most effective model configurations.\n",
        "        hyperparameters = {\n",
        "            'RandomForest': {\n",
        "                # Parameters for RandomForestClassifier, specifying the number of trees (n_estimators),\n",
        "                # the maximum depth of the trees (max_depth), and the minimum number of samples required \n",
        "                # to split an internal node (min_samples_split) and to be at a leaf node (min_samples_leaf).\n",
        "                'randomforestclassifier__n_estimators': [100, 200],\n",
        "                'randomforestclassifier__max_depth': [10, 20],\n",
        "                'randomforestclassifier__min_samples_split': [2, 5],\n",
        "                'randomforestclassifier__min_samples_leaf': [1, 2]\n",
        "            },\n",
        "            'GradientBoosting': {\n",
        "                # Parameters for GradientBoostingClassifier, including the number of boosting stages (n_estimators),\n",
        "                # the learning rate which scales the contribution of each tree (learning_rate), \n",
        "                # and the maximum depth of the regression estimators (max_depth).\n",
        "                'gradientboostingclassifier__n_estimators': [100, 200],\n",
        "                'gradientboostingclassifier__learning_rate': [0.01, 0.05],\n",
        "                'gradientboostingclassifier__max_depth': [3, 4]\n",
        "            },\n",
        "            'KNN': {\n",
        "                # Parameters for KNeighborsClassifier, specifying the number of neighbors to use (n_neighbors),\n",
        "                # the weight function used in prediction (weights), and the power parameter for the Minkowski metric (p).\n",
        "                'kneighborsclassifier__n_neighbors': [3, 5],\n",
        "                'kneighborsclassifier__weights': ['uniform', 'distance'],\n",
        "                'kneighborsclassifier__p': [1, 2]  # 1 for Manhattan distance, 2 for Euclidean distance\n",
        "            },\n",
        "            'NeuralNetwork': {\n",
        "                # Parameters for MLPClassifier (Neural Network), including the size of the hidden layers (hidden_layer_sizes),\n",
        "                # the activation function for the hidden layers (activation), the L2 penalty (regularization term) parameter (alpha),\n",
        "                # and the learning rate schedule (learning_rate).\n",
        "                'mlpclassifier__hidden_layer_sizes': [(50, 50)],\n",
        "                'mlpclassifier__activation': ['relu'],\n",
        "                'mlpclassifier__alpha': [0.0001, 0.001],\n",
        "                'mlpclassifier__learning_rate': ['constant']\n",
        "            },\n",
        "            'LGBM': {\n",
        "                # Parameters for LGBMClassifier, specifying the number of boosting iterations (n_estimators),\n",
        "                # the learning rate (learning_rate), and the maximum depth of the trees (max_depth).\n",
        "                'lgbmclassifier__n_estimators': [100, 200],\n",
        "                'lgbmclassifier__learning_rate': [0.01, 0.05],\n",
        "                'lgbmclassifier__max_depth': [3, 4]\n",
        "            },\n",
        "            'CatBoost': {\n",
        "                # Parameters for CatBoostClassifier, defining the number of training iterations (iterations)\n",
        "                # and the learning rate (learning_rate).\n",
        "                'catboostclassifier__iterations': [500],\n",
        "                'catboostclassifier__learning_rate': [0.01, 0.05]\n",
        "            },\n",
        "            'XGBoost': {\n",
        "                # Parameters for XGBClassifier, including the number of gradient boosted trees (n_estimators),\n",
        "                # the learning rate (learning_rate), and the maximum depth of the trees (max_depth).\n",
        "                'xgbclassifier__n_estimators': [100, 200],\n",
        "                'xgbclassifier__learning_rate': [0.01, 0.05],\n",
        "                'xgbclassifier__max_depth': [3, 4]\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Initialize an empty dictionary to store the scores or performance metrics of the models \n",
        "        # after hyperparameter tuning. This will facilitate comparison and selection of the best performing model.\n",
        "        scores = {}\n",
        "\n",
        "\n",
        "        # Iterate over each model defined in model_definitions.\n",
        "        for name, model in model_definitions.items():\n",
        "            # Log the start of training for the current model.\n",
        "            logging.info(f\"Training {name}...\")\n",
        "            \n",
        "            # Create a pipeline with standard scaling followed by the model. StandardScaler standardizes features\n",
        "            # by removing the mean and scaling to unit variance, which is often beneficial for many models.\n",
        "            pipeline = make_pipeline(StandardScaler(), model)\n",
        "            \n",
        "            # Initialize GridSearchCV with the model's pipeline and its specific hyperparameters.\n",
        "            # StratifiedKFold is used for cross-validation to ensure each fold is a good representative of the whole,\n",
        "            # with n_splits=2 for simplicity and faster computation.\n",
        "            grid_search = GridSearchCV(pipeline, hyperparameters[name], cv=StratifiedKFold(n_splits=2))\n",
        "            \n",
        "            # Fit GridSearchCV on the training data to find the best hyperparameters.\n",
        "            grid_search.fit(self.X_train, self.y_train)\n",
        "            \n",
        "            # Store the best score obtained during the hyperparameter tuning for this model.\n",
        "            scores[name] = grid_search.best_score_\n",
        "            \n",
        "            # Save the best performing model to the models dictionary for later use.\n",
        "            self.models[name] = grid_search.best_estimator_\n",
        "            \n",
        "            # Log the best score and parameters for the current model.\n",
        "            logging.info(f\"{name} Best Score: {grid_search.best_score_}\")\n",
        "            logging.info(f\"{name} Best Params: {grid_search.best_params_}\")\n",
        "\n",
        "\n",
        "        # After tuning, iterate over the models to evaluate them.\n",
        "        for name, model in self.models.items():\n",
        "            # Log the evaluation process start for the current model.\n",
        "            logging.info(f\"Evaluating model: {name}\")\n",
        "\n",
        "            # Determine the appropriate key name for accessing hyperparameters based on the model's name.\n",
        "            # This key name adjustment is necessary because the naming convention in the hyperparameters dictionary\n",
        "            # uses the classifier's class name in lowercase followed by 'classifier', but with exceptions like 'XGBoost'.\n",
        "            if name == \"XGBoost\":\n",
        "                key_name = \"xgbclassifier\"\n",
        "            elif name == \"KNN\":\n",
        "                key_name = \"kneighborsclassifier\"\n",
        "            elif name == \"NeuralNetwork\":\n",
        "                key_name = \"mlpclassifier\"\n",
        "            else:\n",
        "                key_name = name.lower() + \"classifier\"\n",
        "\n",
        "            # Check if the current model has an attribute 'feature_importances_'.\n",
        "            if hasattr(model.named_steps[key_name], 'feature_importances_'):\n",
        "                # Plot the feature importances if the model supports this attribute.\n",
        "                self.plot_feature_importance(model.named_steps[key_name], self.X_train.columns, name)\n",
        "                \n",
        "                # Retrieve the feature importances from the model.\n",
        "                feature_importances = model.named_steps[key_name].feature_importances_\n",
        "                \n",
        "                # Extend the list of top features with the names of the top 10 features based on their importances.\n",
        "                self.top_features.extend(list(self.X_train.columns[np.argsort(feature_importances)[-10:]]))\n",
        "\n",
        "                # Retrieve the fitted model from the pipeline for further analysis.\n",
        "                fitted_model = model.named_steps[key_name]\n",
        "                \n",
        "                # Determine the appropriate SHAP explainer based on the model type.\n",
        "                if isinstance(fitted_model, (RandomForestClassifier, GradientBoostingClassifier, XGBClassifier, LGBMClassifier, CatBoostClassifier, KNeighborsClassifier)):\n",
        "                    # For tree-based models, use the TreeExplainer.\n",
        "                    explainer = shap.TreeExplainer(fitted_model)\n",
        "                elif isinstance(fitted_model, MLPClassifier):\n",
        "                    # For MLPClassifier, TreeExplainer is used with a subset of training data to approximate.\n",
        "                    explainer = shap.TreeExplainer(fitted_model, self.X_train.sample(n=100))\n",
        "                else:\n",
        "                    # For other model types, use KernelExplainer as a more general approach, again with a sample.\n",
        "                    explainer = shap.KernelExplainer(fitted_model.predict, self.X_train.sample(n=100))\n",
        "                \n",
        "                # Clear the current figure to ensure a clean slate for SHAP visualization.\n",
        "                plt.clf()\n",
        "\n",
        "                #explainer = shap.TreeExplainer(fitted_model)\n",
        "                \n",
        "                # Compute SHAP values for the training set.\n",
        "                shap_values = explainer.shap_values(self.X_train)\n",
        "                \n",
        "                # Generate and display a summary plot of the SHAP values for all features across all samples in the X_train.\n",
        "                shap.summary_plot(shap_values, self.X_train, plot_type=\"bar\")\n",
        "                \n",
        "                # Save the SHAP summary plot to a file for later reference.\n",
        "                filename = f\"{name}_shap_plot.png\"\n",
        "                plt.savefig(filename)\n",
        "                plt.show()\n",
        "\n",
        "            # Evaluate using LIME\n",
        "            # Invoke the LIME explanation method for the current model to provide interpretable explanations\n",
        "            # for model predictions on individual instances. This is crucial for understanding how model decisions are made.\n",
        "            self.explain_with_lime(model, self.X_train, self.X_test, name)\n",
        "\n",
        "        # After evaluating all models and collecting the top features from each, ensure the list of top features is unique by converting it to a set and back to a list.\n",
        "        self.top_features = list(set(self.top_features))\n",
        "        # Print the unique top features for inspection. These features are deemed most important across all models.\n",
        "        print(self.top_features)\n",
        "\n",
        "    def evaluate_model(self):\n",
        "        # Iterate over each model stored in the self.models dictionary.\n",
        "        for name, model in self.models.items():\n",
        "            # Generate predictions for the test set using the current model.\n",
        "            predictions = model.predict(self.X_test)\n",
        "            # Print a classification report for each model, providing a detailed performance analysis\n",
        "            # including precision, recall, f1-score, and support for each class.\n",
        "            print(f\"Classification Report ({name}):\\n{classification_report(self.y_test, predictions)}\")\n",
        "        \n",
        "        # Rank the models based on their performance on the test set and select the top 5.\n",
        "        # This ranking uses the model's score method, which typically defaults to accuracy for classifiers.\n",
        "        top_models = sorted(self.models.items(), key=lambda x: x[1].score(self.X_test, self.y_test), reverse=True)[:5]\n",
        "        \n",
        "        # Create an ensemble of the top 5 models using VotingClassifier with 'hard' voting, which means that the predicted class label will be the class that receives the most votes.\n",
        "        ensemble = VotingClassifier(estimators=top_models, voting='hard')\n",
        "        \n",
        "        # Train the ensemble model using only the top features identified as most important across all models.\n",
        "        # This can potentially improve the model's performance by focusing on the most relevant information.\n",
        "        ensemble.fit(self.X_test[self.top_features], self.y_test)\n",
        "        \n",
        "        # Predict the class labels for the test set using the ensemble model.\n",
        "        predictions = ensemble.predict(self.X_test[self.top_features])\n",
        "        \n",
        "        # Print a classification report for the ensemble model, comparing its performance to the individual models.\n",
        "        print(f\"Classification Report (Ensemble):\\n{classification_report(self.y_test, predictions)}\")\n",
        "\n",
        "    def evaluate_multiple_subsets(self, feature_subsets):\n",
        "        # Initialize a dictionary to hold evaluation scores for different feature subsets.\n",
        "        scores_by_subset = {}\n",
        "\n",
        "        # Iterate over each subset of features defined in `feature_subsets`.\n",
        "        for subset_name, features in feature_subsets.items():\n",
        "            # Evaluate the model performance using the current subset of features.\n",
        "            # This function trains the model on the subset and returns various performance metrics.\n",
        "            scores = self.evaluate_subset(subset_name, features, self.X_train, self.X_test, self.y_train, self.y_test, feature_subsets)\n",
        "            \n",
        "            # Store the scores for the current subset by its name.\n",
        "            scores_by_subset[subset_name] = scores\n",
        "\n",
        "            # Plot the impact of the current feature subset on model performance.\n",
        "            self.plot_subset_impact(subset_name, *scores)\n",
        "\n",
        "            # An optional step to display the metrics for the current subset\n",
        "            #self.display_metrics(*scores)\n",
        "\n",
        "        # Return the collected scores for all subsets.\n",
        "        return scores_by_subset\n",
        "\n",
        "    def evaluate_subset_allfeatures(self):\n",
        "        # Initialize a dictionary to store the scores when all features are used.\n",
        "        scores_with = {}\n",
        "\n",
        "        # Train the models defined in the class. This step assumes that `train_model` not only initializes but also tunes the models, making them ready for performance evaluation.\n",
        "        self.train_model()\n",
        "\n",
        "        # Iterate over each trained model to evaluate its performance using all available features.\n",
        "        for model_name, model in self.models.items():\n",
        "            # Print the name of the model currently being evaluated.\n",
        "            print(model_name)\n",
        "\n",
        "            # Initialize a dictionary to store ROC AUC related metrics for the current model.\n",
        "            self.roc_auc[model_name] = {}\n",
        "\n",
        "            # Fit the model on the training set and predict the test set.\n",
        "            model.fit(self.X_train, self.y_train)\n",
        "            y_pred = model.predict(self.X_test)\n",
        "\n",
        "            # Calculate various performance metrics including accuracy, precision, recall, and F1 score.\n",
        "            accuracy = accuracy_score(self.y_test, y_pred)\n",
        "            precision = precision_score(self.y_test, y_pred)\n",
        "            recall = recall_score(self.y_test, y_pred)\n",
        "            f1 = f1_score(self.y_test, y_pred)\n",
        "            scores_with[model_name] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1 Score': f1}\n",
        "\n",
        "            # For ROC AUC calculation, predict probabilities and compute the curve and area.\n",
        "            y_pred_prob = model.predict_proba(self.X_test)[:, 1]\n",
        "            fpr, tpr, thresholds = roc_curve(self.y_test, y_pred_prob, pos_label=1)\n",
        "            roc_auc_area = roc_auc_score(self.y_test, y_pred_prob)\n",
        "\n",
        "            # Store the ROC curve metrics and area under the curve (AUC) for the current model.\n",
        "            self.roc_auc[model_name]['fpr'] = fpr\n",
        "            self.roc_auc[model_name]['tpr'] = tpr\n",
        "            self.roc_auc[model_name]['thresholds'] = thresholds\n",
        "            self.roc_auc[model_name]['roc_auc'] = roc_auc_area\n",
        "\n",
        "        # Return the scores for all models when all features are used.\n",
        "        return scores_with\n",
        "\n",
        "\n",
        "    def evaluate_subset(self, subset_name, features, X_train, X_test, y_train, y_test, feature_subsets):\n",
        "        \"\"\"\n",
        "        Evaluate model performance using different configurations of feature subsets to understand\n",
        "        the impact of certain features on model accuracy and other metrics.\n",
        "        \"\"\"\n",
        "        # Initialize dictionaries to store scores for different feature configurations.\n",
        "        scores_with_traditional = {}\n",
        "        scores_with = {}\n",
        "        scores_without = {}\n",
        "        scores_subset_only = {}\n",
        "        scores_subset_and_original = {}\n",
        "\n",
        "        # Aggregate all unique features across all subsets for analysis.\n",
        "        all_subset_features = set()\n",
        "        for feature_subset in feature_subsets.values():\n",
        "            all_subset_features.update(feature_subset)\n",
        "\n",
        "        # Identify features present in the original dataset but not in any subset.\n",
        "        original_minus_subset = [feature for feature in X_train.columns if feature not in all_subset_features]\n",
        "\n",
        "        for model_name, model in self.models.items():\n",
        "\n",
        "            # Evaluate using all features.\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_test)\n",
        "            # Store accuracy, precision, recall, and F1 score.\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            precision = precision_score(y_test, y_pred)\n",
        "            recall = recall_score(y_test, y_pred)\n",
        "            f1 = f1_score(y_test, y_pred)\n",
        "            scores_with[model_name] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1 Score': f1}\n",
        "\n",
        "            # Evaluate using traditional features (original features minus subset features).\n",
        "            model.fit(X_train[original_minus_subset], y_train)\n",
        "            y_pred_original_minus_subset = model.predict(X_test[original_minus_subset])\n",
        "            accuracy = accuracy_score(y_test, y_pred_original_minus_subset)\n",
        "            precision = precision_score(y_test, y_pred_original_minus_subset)\n",
        "            recall = recall_score(y_test, y_pred_original_minus_subset)\n",
        "            f1 = f1_score(y_test, y_pred_original_minus_subset)\n",
        "            scores_with_traditional[model_name] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1 Score': f1}\n",
        "\n",
        "            # Evaluate without the specific subset of features.\n",
        "            X_train_sub = X_train.drop(features, axis=1)\n",
        "            X_test_sub = X_test.drop(features, axis=1)\n",
        "            model.fit(X_train_sub, y_train)\n",
        "            y_pred_sub = model.predict(X_test_sub)\n",
        "            accuracy = accuracy_score(y_test, y_pred_sub)\n",
        "            precision = precision_score(y_test, y_pred_sub)\n",
        "            recall = recall_score(y_test, y_pred_sub)\n",
        "            f1 = f1_score(y_test, y_pred_sub)\n",
        "            scores_without[model_name] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1 Score': f1}\n",
        "\n",
        "            # Evaluate with only the subset of features.\n",
        "            model.fit(X_train[features], y_train)\n",
        "            y_pred_subset = model.predict(X_test[features])\n",
        "            accuracy = accuracy_score(y_test, y_pred_subset)\n",
        "            precision = precision_score(y_test, y_pred_subset)\n",
        "            recall = recall_score(y_test, y_pred_subset)\n",
        "            f1 = f1_score(y_test, y_pred_subset)\n",
        "            scores_subset_only[model_name] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1 Score': f1}\n",
        "\n",
        "            # Evaluate with a combination of the subset and traditional features.\n",
        "            combined_features = list (set(features + list(original_minus_subset)))\n",
        "            model.fit(X_train[combined_features], y_train)\n",
        "            y_pred_subset_and_original = model.predict(X_test[combined_features])\n",
        "            accuracy = accuracy_score(y_test, y_pred_subset_and_original)\n",
        "            precision = precision_score(y_test, y_pred_subset_and_original)\n",
        "            recall = recall_score(y_test, y_pred_subset_and_original)\n",
        "            f1 = f1_score(y_test, y_pred_subset_and_original)\n",
        "            scores_subset_and_original[model_name] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1 Score': f1}\n",
        "\n",
        "\n",
        "            # Check for performance improvement\n",
        "            #print(f\"For {model_name}, Original Score: {scores_with[model_name]:.4f}, Without Subset: {scores_without[model_name]:.4f}, RFE Score: {scores_with_rfe[model_name]:.4f}, L1 Score: {scores_with_l1[model_name]:.4f}, Tree Score: {scores_with_tree[model_name]:.4f}\")\n",
        "            # Print a header for the evaluation metrics specific to the model and feature subset being analyzed.\n",
        "            print(f\"Evaluation Metrics for Model: {model_name} using '{subset_name}' subset of features\")\n",
        "            # Display the accuracy when using traditional features only.\n",
        "            print(f\"  - Accuracy with traditional features: {scores_with_traditional[model_name]}\")\n",
        "            # Display the accuracy when the model is trained with all available features.\n",
        "            print(f\"  - Accuracy with all features: {scores_with[model_name]}\")\n",
        "            # Display the accuracy when the model is trained with a combination of the evaluated subset and traditional features.\n",
        "            # This shows whether integrating the subset with traditional features offers any performance improvement.\n",
        "            print(f\"  - Accuracy with '{subset_name}' subset + traditional features: {scores_subset_and_original[model_name]}\")\n",
        "\n",
        "        # Return the dictionaries containing scores for all configurations tested.\n",
        "        # These scores include models trained with traditional features, all features, without the subset, with the subset only, and with a combination of the subset and traditional features.\n",
        "        return scores_with_traditional, scores_with, scores_without, scores_subset_only, scores_subset_and_original\n",
        "\n",
        "\n",
        "    def add_values_on_bars(self, ax, values, offset=0.01):\n",
        "        \"\"\"\n",
        "        Annotate bar plots with their numerical values for better readability.\n",
        "\n",
        "        Parameters:\n",
        "        - ax: The matplotlib axes object where the bars are plotted.\n",
        "        - values: A list of values corresponding to each bar to be annotated.\n",
        "        - offset: A vertical offset to adjust the position of the annotation above the bar.\n",
        "        \"\"\"\n",
        "        for i, v in enumerate(values):\n",
        "            # Annotate each bar with its respective value, adjusting the position for visibility.\n",
        "            # 'ha' and 'va' control the alignment of the text.\n",
        "            ax.text(i, v + offset, f\"{v:.2f}\", fontsize=10, ha='center', va='bottom')\n",
        "\n",
        "\n",
        "    def plot_subset_impact(self, subset_name, scores_with_traditional, scores_with, scores_without, scores_subset_only, scores_subset_and_original):\n",
        "        \"\"\"\n",
        "        Visualize the impact of different feature subsets on model performance metrics.\n",
        "\n",
        "        Parameters:\n",
        "        - subset_name: Name of the feature subset being evaluated.\n",
        "        - scores_*: Dictionaries containing scores for models trained with different feature configurations.\n",
        "        \"\"\"\n",
        "        # Retrieve model names as they are consistent across all score dictionaries.\n",
        "        model_names = list(scores_with_traditional.keys())\n",
        "        print(\"Inside the Plot Subset impact Names\")\n",
        "\n",
        "        # Iterate over a set of metrics to create separate bar plots for each.\n",
        "        for metric in ['Accuracy','Precision','Recall','F1 Score']:\n",
        "            plt.figure(figsize=(20, 6))\n",
        "            barWidth = 0.1  # Set the width of each bar.\n",
        "            # Calculate the positions of each group of bars on the X axis.\n",
        "            r1 = np.arange(len(model_names))\n",
        "            r2 = [x + barWidth for x in r1]\n",
        "            r3 = [x + barWidth for x in r2]\n",
        "            r4 = [x + barWidth for x in r3]\n",
        "            r5 = [x + barWidth for x in r4]\n",
        "\n",
        "            '''\n",
        "            r6 = [x + barWidth for x in r5]\n",
        "            r7 = [x + barWidth for x in r6]\n",
        "            r8 = [x + barWidth for x in r7]\n",
        "            r9 = [x + barWidth for x in r8]\n",
        "             '''\n",
        "            # Plot bars for each feature configuration across all models for the current metric.\n",
        "            y_values1 = [scores_with[item][metric] for item in scores_with]\n",
        "            plt.bar(r1, y_values1, color='b', width=barWidth, edgecolor='grey', label='With All Features')\n",
        "            y_values2 = [scores_with_traditional[item][metric] for item in scores_with_traditional]\n",
        "            plt.bar(r2, y_values2, color='black', width=barWidth, edgecolor='grey', label='With Traditional Features Only')\n",
        "            y_values3 = [scores_without[item][metric] for item in scores_without]\n",
        "            plt.bar(r3, y_values3, color='r', width=barWidth, edgecolor='grey', label='Without Subset')\n",
        "            y_values4 = [scores_subset_only[item][metric] for item in scores_subset_only]\n",
        "            plt.bar(r4, y_values4, color='g', width=barWidth, edgecolor='grey', label='With Subset Only')\n",
        "            y_values5 = [scores_subset_and_original[item][metric] for item in scores_subset_and_original]\n",
        "            plt.bar(r5, y_values5, color='y', width=barWidth, edgecolor='grey', label='With Subset and Traditional')\n",
        "\n",
        "            '''\n",
        "            y_values6 = [scores_with_rfe[item]['Accuracy'] for item in scores_with_rfe]\n",
        "            plt.bar(r6, y_values6, color='c', width=barWidth, edgecolor='grey', label='With RFE')\n",
        "            y_values7 = [scores_with_l1[item]['Accuracy'] for item in scores_with_l1]\n",
        "            plt.bar(r7, y_values7, color='m', width=barWidth, edgecolor='grey', label='With L1')\n",
        "            y_values8 = [scores_with_tree[item]['Accuracy'] for item in scores_with_tree]\n",
        "            plt.bar(r8, y_values8, color='orange', width=barWidth, edgecolor='grey', label='With Tree')\n",
        "            y_values9 = [scores_with_anova[item]['Accuracy'] for item in scores_with_anova]\n",
        "            plt.bar(r9, y_values9, color='purple', width=barWidth, edgecolor='grey', label='With ANOVA')\n",
        "            '''\n",
        "            # Adding numerical values on top of the bars for clarity.\n",
        "            ax = plt.gca() \n",
        "            values_lists = [y_values1, y_values2, y_values3, y_values4, y_values5] # List of values for each group of bars.\n",
        "            x_values = [r1, r2, r3, r4, r5] # Corresponding x positions for each group.\n",
        "\n",
        "            # Debug prints to verify the values and positions.\n",
        "            print(values_lists)\n",
        "            print('x values:',x_values)\n",
        "            # Loop through each group of bars to annotate them.\n",
        "            for i in range(len(x_values)):\n",
        "                print('In loop:',i)\n",
        "                print(x_values[i])\n",
        "                print(values_lists[i])\n",
        "                # Annotate each bar with its value for better readability.\n",
        "                for category, value in zip(x_values[i], values_lists[i]):\n",
        "                    ax.text(category, value, \"{:.2f}\".format(value), ha='center', va='bottom')\n",
        "\n",
        "            # Set labels for the x and y axes with specific font properties.\n",
        "            plt.xlabel('Models', fontweight='bold', fontsize=16)\n",
        "            plt.ylabel(f'{metric}', fontsize=16)\n",
        "            # Title for the plot that includes the subset name and the method used for model evaluation.\n",
        "            plt.title(f'Impact of {subset_name} Features on Model Performance [Method: {self.mode}]', fontsize=20)\n",
        "            # Set x-ticks to be in the middle of each group of bars.\n",
        "            plt.xticks([r + 4 * barWidth for r in range(len(r1))], model_names)  # Adjust the multiplier based on the number of bars\n",
        "\n",
        "            #offset = 0.01  # Adjust this value for vertical placement\n",
        "            #for values in values_lists:\n",
        "            #    self.add_values_on_bars(plt.gca(), values, offset)\n",
        "\n",
        "            plt.legend()\n",
        "            plt.savefig(f\"subset_impact_metric_{metric}_{subset_name}.png\")\n",
        "            #plt.show()\n",
        "            plt.close()\n",
        "\n",
        "\n",
        "    def display_metrics(self, scores_with_traditional, scores_with, scores_without, scores_subset_only, scores_subset_and_original):\n",
        "        \"\"\"\n",
        "        Prints out the performance metrics for models trained with different feature configurations.\n",
        "\n",
        "        Parameters:\n",
        "        - scores_with_traditional: Dictionary containing model scores using traditional features.\n",
        "        - scores_with: Dictionary containing model scores using all available features.\n",
        "        - scores_without: Dictionary containing model scores excluding the specified feature subset.\n",
        "        - scores_subset_only: Dictionary containing model scores using only the specified feature subset.\n",
        "        - scores_subset_and_original: Dictionary containing model scores using both the specified subset and traditional features.\n",
        "        \"\"\"\n",
        "        print(\"\\nModel Performance Metrics:\")\n",
        "        for model_name in scores_with_traditional.keys():\n",
        "            # For each model, print out the accuracy for different feature configurations.\n",
        "            print(f\"\\nFor the model: {model_name}\")\n",
        "            print(f\"  - Accuracy with traditional features: {scores_with_traditional[model_name]:.4f}\")\n",
        "            print(f\"  - Accuracy with all features: {scores_with[model_name]:.4f}\")\n",
        "            print(f\"  - Accuracy without the subset: {scores_without[model_name]:.4f}\")\n",
        "            print(f\"  - Accuracy with only the subset: {scores_subset_only[model_name]:.4f}\")\n",
        "            print(f\"  - Accuracy with the subset + original features: {scores_subset_and_original[model_name]:.4f}\")\n",
        "\n",
        "\n",
        "    def predict(self, data):\n",
        "        \"\"\"\n",
        "        Predicts the outcome using an ensemble model composed of all the trained models.\n",
        "\n",
        "        Parameters:\n",
        "        - data: The dataset to predict outcomes for, expected to contain the top features.\n",
        "        \n",
        "        Returns:\n",
        "        - The predicted outcomes based on the ensemble of models.\n",
        "        \"\"\"\n",
        "\n",
        "        # Create an ensemble model using VotingClassifier with 'hard' voting strategy.\n",
        "        # 'Hard' voting means that the predicted class label is the one that has received the most votes.\n",
        "        ensemble = VotingClassifier(estimators=list(self.models.items()), voting='hard')\n",
        "        # Predict the outcomes for the provided data using the ensemble model.\n",
        "        # It uses the `top_features` determined during the model training phase for prediction.\n",
        "        return ensemble.predict(data[self.top_features])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3NyJqeIq0i6i",
        "outputId": "11658562-a677-4c94-c9c9-26a6173b87e5"
      },
      "outputs": [],
      "source": [
        "# Instantiate the StrokePrediction class with a specified data path and the mode set to 'undersample'.\n",
        "# The 'undersample' mode indicates that the class will balance the dataset by undersampling the majority class during the preprocessing phase to address class imbalance.\n",
        "predictor = StrokePrediction(path,mode='undersample')\n",
        "#X_train, X_test, y_train, y_test = predictor.preprocess_data()\n",
        "\n",
        "# Train models using the undersampled dataset. This step involves fitting the models defined within the StrokePrediction class on the training data. \n",
        "predictor.train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRiweq8y_Fyg"
      },
      "outputs": [],
      "source": [
        "# Evaluate the trained models on the test dataset to assess their performance.\n",
        "predictor.evaluate_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "GIqD9mfQy3NC",
        "outputId": "f517e46b-6142-4091-d979-976875fec809"
      },
      "outputs": [],
      "source": [
        "# Define a dictionary mapping thematic categories to lists of features. Each category represents a different aspect\n",
        "# of the individuals' economic, neighborhood, personal, demographic, racial, ethnic, social, and mental health characteristics.\n",
        "# This organization helps in analyzing the impact of various feature groups on the model's ability to predict strokes.\n",
        "feature_subsets = {\n",
        "    \"Economic\": ['Working','Working_Last_Week','Regular_Healthcare_Provider'],\n",
        "    \"Neighbourhood\": [ 'Urban_Rural','Household_Ownership'],\n",
        "    \"Personal\":['Problem_Handling_Ability','Day_to_Day_Demands_Ability', 'Perceived_Health', 'Perceived_Life_Stress'],\n",
        "    \"Demographic\":['DHH_SEX', 'DHH_AGE', 'DHH_MS'],\n",
        "    \"Race\": ['White','Chinese','South_Asian','Black','Filipino','Latin_America',\n",
        "            'South_East_Asian','Arab','West_Asian','Japanese','Korean'],\n",
        "    \"Ethnicity\": ['Canadian','French','English','German','Scottish','Irish','Italian',\n",
        "            'Ukrainian','Dutch','Chinese_Ethnic','Jewish','Polish','South_Asian_Ethnic','Norwegian', 'Welsh','Swedish','Other','Metis','Inuit'],\n",
        "    \"Social\":['Food_Security', 'Food_Affordability', 'Sense_of_Community_Belonging','Level_of_Education'],\n",
        "    \"Mental_Health\":['Mood_Disorder','Anxiety_Disorder','Mental_Health_Consult_12mo']\n",
        "\n",
        "    # Add more subsets as needed\n",
        "}\n",
        "\n",
        "# Evaluate the performance of models for each defined feature subset within the `feature_subsets` dictionary.\n",
        "# This step involves training and testing the models on each subset of features and then aggregating the scores\n",
        "# to understand which feature groups are most predictive of strokes. The method `evaluate_multiple_subsets` is\n",
        "# handles the training, testing, and evaluation process, returning a dictionary of scores keyed by subset name.\n",
        "scores_by_subset = predictor.evaluate_multiple_subsets(feature_subsets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iR5VZUXZ0noT",
        "outputId": "8012d4b0-f98c-4be3-8477-48fe34434de2"
      },
      "outputs": [],
      "source": [
        "# Evaluate models using all available features.\n",
        "predictor.evaluate_subset_allfeatures()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "ns-KDcJmy53I",
        "outputId": "b5bea66d-1ce4-4826-cfed-3b51c1c6697a"
      },
      "outputs": [],
      "source": [
        "# Ensure matplotlib plots are displayed inline in the Jupyter notebook.\n",
        "%matplotlib inline\n",
        "\n",
        "# Prepare for a new figure to plot the ROC curves.\n",
        "plt.close()\n",
        "\n",
        "# Plot the ROC curve for each model. Each curve represents the model's performance in distinguishing between the positive and negative classes across different thresholds.\n",
        "plt.plot(predictor.roc_auc['RandomForest']['fpr'], predictor.roc_auc['RandomForest']['tpr'], label='Random Forest (%.2f)'%predictor.roc_auc['RandomForest']['roc_auc'])\n",
        "plt.plot(predictor.roc_auc['GradientBoosting']['fpr'], predictor.roc_auc['GradientBoosting']['tpr'], label='GradientBoosting (%.2f)'%predictor.roc_auc['GradientBoosting']['roc_auc'])\n",
        "plt.plot(predictor.roc_auc['KNN']['fpr'], predictor.roc_auc['KNN']['tpr'], label='KNN (%.2f)'%predictor.roc_auc['KNN']['roc_auc'])\n",
        "plt.plot(predictor.roc_auc['LGBM']['fpr'], predictor.roc_auc['LGBM']['tpr'], label='LGBM (%.2f)'%predictor.roc_auc['LGBM']['roc_auc'])\n",
        "plt.plot(predictor.roc_auc['NeuralNetwork']['fpr'], predictor.roc_auc['NeuralNetwork']['tpr'], label='NeuralNetwork (%.2f)'%predictor.roc_auc['NeuralNetwork']['roc_auc'])\n",
        "plt.plot(predictor.roc_auc['CatBoost']['fpr'], predictor.roc_auc['CatBoost']['tpr'], label='CatBoost (%.2f)'%predictor.roc_auc['CatBoost']['roc_auc'])\n",
        "plt.plot(predictor.roc_auc['XGBoost']['fpr'], predictor.roc_auc['XGBoost']['tpr'], label='XGBoost (%.2f)'%predictor.roc_auc['XGBoost']['roc_auc'])\n",
        "\n",
        "# Plot the line for a random classifier as a baseline comparison. The diagonal dashed line represents a model that predicts classes in a completely random manner, with AUC = 0.5.\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random classifier')\n",
        "# Set the labels and title for the ROC plot. \n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve (All features)[Method: Undersampling]')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch_test",
      "language": "python",
      "name": "pytorch_test"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
